{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSP_CCE_Proj2_20202021_Hate_Speech_Detection_Neural_Learning_Models_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrCuCYE3b7Zs"
      },
      "source": [
        "# Hate speech Detection using CNN\n",
        "In this notebook, we conduct a preliminary experiment on the detection of hate speech in Arabic tweets as part of our participation in the Hate Speech Detection subtask in [OSACT4 workshop](http://edinburghnlp.inf.ed.ac.uk/workshops/OSACT4/).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TwgKlI1te-w",
        "outputId": "d54b96e1-3a44-4696-94ed-7e9ab816b86a"
      },
      "source": [
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSw0od87C8-"
      },
      "source": [
        "from keras.layers import Embedding, Dense, Dropout, Input, LSTM, Bidirectional,GRU\n",
        "from keras.layers import MaxPooling1D, Conv1D, Flatten\n",
        "from keras.preprocessing import sequence, text\n",
        "from keras.models import Model\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn import preprocessing\n",
        "from time import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import (\n",
        "    classification_report as creport\n",
        ")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6_dt2cDtobO"
      },
      "source": [
        "# Data and AraVec2.0 (pre-trained word embeddings model) Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mn0KDJg-o5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b9164e7-4f0a-4fc6-bd52-469645a5376b"
      },
      "source": [
        "#pre-trained word embedding: https://github.com/bakrianoo/aravec/tree/master/AraVec%202.0\n",
        "\"\"\"\n",
        "Citation:\n",
        "Abu Bakr Soliman, Kareem Eisa, and Samhaa R. El-Beltagy, â€œAraVec:\n",
        "A set of Arabic Word Embedding Models for use in Arabic NLPâ€,\n",
        "in proceedings of the 3rd International Conference on \n",
        "Arabic Computational Linguistics (ACLing 2017), Dubai, UAE, 2017.\n",
        "\"\"\"\n",
        "! unzip '/content/drive/My Drive/tweets_sg_300.zip'  "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/tweets_sg_300.zip\n",
            "  inflating: tweets_sg_300           \n",
            "  inflating: tweets_sg_300.trainables.syn1neg.npy  \n",
            "  inflating: tweets_sg_300.wv.vectors.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3kKX6Ra-p1d"
      },
      "source": [
        "# Word_embedding_path\n",
        "embedding_path = '/content/tweets_sg_300'           #Twitter-Skipgram model-300d(trained on 77,600,000 Arabic tweets)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRSY6aVACVF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "0bf4b5c0-48e1-4e27-91c5-20d704a4decf"
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/My Drive/train_data.csv')\n",
        "train_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ ÙŠØ§Ø±Ø¨ ÙÙˆØ² Ù…Ù‡Ù… ÙŠØ§ Ø²Ù…Ø§Ù„Ùƒ.. ÙƒÙ„ Ø§Ù„Ø¯Ø¹Ù… Ù„ÙŠÙƒÙ…...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ÙØ¯ÙˆÙ‡ ÙŠØ§ Ø¨Ø®Øª ÙØ¯ÙˆÙ‡ ÙŠØ§ Ø²Ù…Ù† ÙˆØ§Ø­Ø¯ Ù…Ù†ÙƒÙ… ÙŠØ¬ÙŠØ¨Ù‡</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @USER: ÙŠØ§ Ø±Ø¨ ÙŠØ§ ÙˆØ§Ø­Ø¯ ÙŠØ§ Ø£Ø­Ø¯ Ø¨Ø­Ù‚ ÙŠÙˆÙ… Ø§Ù„Ø§Ø­Ø¯ Ø§...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: #Ù‡ÙˆØ§_Ø§Ù„Ø­Ø±ÙŠØ© ÙŠØ§ ÙˆØ¬Ø¹ Ù‚Ù„Ø¨ÙŠ Ø¹Ù„ÙŠÙƒÙŠ ÙŠØ§ Ø§Ù…ÙŠ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ÙŠØ§ Ø¨ÙƒÙˆÙ† Ø¨Ø­ÙŠØ§ØªÙƒ Ø§Ù„Ø£Ù‡Ù… ÙŠØ§ Ø¥Ù…Ø§ Ù…Ø§ Ø¨Ø¯ÙŠ Ø£ÙƒÙˆÙ† ğŸ¼</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6995</th>\n",
              "      <td>@USER ÙŠØ§ Ø­Ù…Ø§Ø± ØŒ ÙŠØ§ Ø¬Ø§Ù‡Ù„ ØŒ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø§Ø·Ù„ Ù…Ø§ Ø¨ØªØªØ­Ø³...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6996</th>\n",
              "      <td>RT @USER: @USER ÙƒÙ„ Ø²Ù‚ ÙŠØ§ Ø·Ø§Ù‚ÙŠØ© ÙŠØ§ ÙˆØ§Ø·ÙŠ ÙŠØ§ Ø­Ù‚ÙŠØ±...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6997</th>\n",
              "      <td>@USER&lt;LF&gt;ÙŠØ§ ÙƒØ¨ÙŠØ± ÙŠØ§ Ù…Ù…ØªØ¹ ÙŠØ§ Ù†Ø¬Ù… Ù„Ø§Ø¨Ø¯ Ø£Ù† ØªØ¹ÙŠ Ø¬ÙŠ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6998</th>\n",
              "      <td>ÙŠØ§ Ø±Ø¨ Ø§Ù„Ø§ØªØ­Ø§Ø¯ ÙŠÙÙˆØ² ÙŠØ§ Ø±Ø¨. ğŸ˜­ğŸ˜­ #Ø§Ù„Ø§ØªØ­Ø§Ø¯_Ø§Ù„Ù†ØµØ±</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6999</th>\n",
              "      <td>ÙŠØ¹Ø² Ø¹Ù„ÙŠØ§ Ø§Ø¯Ø®Ù„ Ø§Ù„Ø´Ø§Ø±Ø¹ ÙˆÙ…Ù„Ø§Ù‚ÙŠÙƒØ´ ÙˆØ§Ù‚Ù Ù…Ø³ØªÙ†ÙŠÙ†ÙŠ ÙÙŠ ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet Offensive    Hate\n",
              "0     Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ ÙŠØ§Ø±Ø¨ ÙÙˆØ² Ù…Ù‡Ù… ÙŠØ§ Ø²Ù…Ø§Ù„Ùƒ.. ÙƒÙ„ Ø§Ù„Ø¯Ø¹Ù… Ù„ÙŠÙƒÙ…...   NOT_OFF  NOT_HS\n",
              "1               ÙØ¯ÙˆÙ‡ ÙŠØ§ Ø¨Ø®Øª ÙØ¯ÙˆÙ‡ ÙŠØ§ Ø²Ù…Ù† ÙˆØ§Ø­Ø¯ Ù…Ù†ÙƒÙ… ÙŠØ¬ÙŠØ¨Ù‡   NOT_OFF  NOT_HS\n",
              "2     RT @USER: ÙŠØ§ Ø±Ø¨ ÙŠØ§ ÙˆØ§Ø­Ø¯ ÙŠØ§ Ø£Ø­Ø¯ Ø¨Ø­Ù‚ ÙŠÙˆÙ… Ø§Ù„Ø§Ø­Ø¯ Ø§...       OFF      HS\n",
              "3     RT @USER: #Ù‡ÙˆØ§_Ø§Ù„Ø­Ø±ÙŠØ© ÙŠØ§ ÙˆØ¬Ø¹ Ù‚Ù„Ø¨ÙŠ Ø¹Ù„ÙŠÙƒÙŠ ÙŠØ§ Ø§Ù…ÙŠ...   NOT_OFF  NOT_HS\n",
              "4             ÙŠØ§ Ø¨ÙƒÙˆÙ† Ø¨Ø­ÙŠØ§ØªÙƒ Ø§Ù„Ø£Ù‡Ù… ÙŠØ§ Ø¥Ù…Ø§ Ù…Ø§ Ø¨Ø¯ÙŠ Ø£ÙƒÙˆÙ† ğŸ¼   NOT_OFF  NOT_HS\n",
              "...                                                 ...       ...     ...\n",
              "6995  @USER ÙŠØ§ Ø­Ù…Ø§Ø± ØŒ ÙŠØ§ Ø¬Ø§Ù‡Ù„ ØŒ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨Ø§Ø·Ù„ Ù…Ø§ Ø¨ØªØªØ­Ø³...       OFF  NOT_HS\n",
              "6996  RT @USER: @USER ÙƒÙ„ Ø²Ù‚ ÙŠØ§ Ø·Ø§Ù‚ÙŠØ© ÙŠØ§ ÙˆØ§Ø·ÙŠ ÙŠØ§ Ø­Ù‚ÙŠØ±...       OFF  NOT_HS\n",
              "6997  @USER<LF>ÙŠØ§ ÙƒØ¨ÙŠØ± ÙŠØ§ Ù…Ù…ØªØ¹ ÙŠØ§ Ù†Ø¬Ù… Ù„Ø§Ø¨Ø¯ Ø£Ù† ØªØ¹ÙŠ Ø¬ÙŠ...   NOT_OFF  NOT_HS\n",
              "6998        ÙŠØ§ Ø±Ø¨ Ø§Ù„Ø§ØªØ­Ø§Ø¯ ÙŠÙÙˆØ² ÙŠØ§ Ø±Ø¨. ğŸ˜­ğŸ˜­ #Ø§Ù„Ø§ØªØ­Ø§Ø¯_Ø§Ù„Ù†ØµØ±   NOT_OFF  NOT_HS\n",
              "6999  ÙŠØ¹Ø² Ø¹Ù„ÙŠØ§ Ø§Ø¯Ø®Ù„ Ø§Ù„Ø´Ø§Ø±Ø¹ ÙˆÙ…Ù„Ø§Ù‚ÙŠÙƒØ´ ÙˆØ§Ù‚Ù Ù…Ø³ØªÙ†ÙŠÙ†ÙŠ ÙÙŠ ...   NOT_OFF  NOT_HS\n",
              "\n",
              "[7000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5frVvfiL72D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "69420dec-4600-4290-c18b-bcb8393ca059"
      },
      "source": [
        "dev_data = pd.read_csv('/content/drive/My Drive/dev_data.csv')\n",
        "dev_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ÙÙ‰ Ø­Ø§Ø¬Ø§Øª Ù…ÙŠÙ†ÙØ¹Ø´ Ù†Ù„ÙØª Ù†Ø¸Ø±ÙƒÙˆØ§ Ù„ÙŠÙ‡Ø§ Ø²Ù‰ Ø§Ù„Ø§ØµÙˆÙ„ ÙƒØ¯Ù‡...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RT @USER: ÙˆØ¹ÙŠÙˆÙ† ØªÙ†Ø§Ø¯ÙŠÙ†Ø§ ØªØ­Ø§ÙŠÙ„ ÙÙŠÙ†Ø§ Ùˆ Ù†Ù‚ÙˆÙ„ ÙŠØ§ Ø¹...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ÙŠØ§ Ø¨Ù„Ø§Ø¯ÙŠ ÙŠØ§ Ø£Ù… Ø§Ù„Ø¨Ù„Ø§Ø¯ ÙŠØ§ Ø¨Ù„Ø§Ø¯ÙŠ Ø¨Ø­Ø¨Ùƒ ÙŠØ§ Ù…ØµØ± Ø¨Ø­Ø¨...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: ÙŠØ§ Ø±Ø¨ ÙŠØ§ Ù‚ÙˆÙŠ ÙŠØ§ Ù…Ø¹ÙŠÙ† Ù…Ø¯Ù‘Ù†ÙŠ Ø¨Ø§Ù„Ù‚ÙˆØ© Ùˆ ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @USER: Ø±Ø­Ù…Ùƒ Ø§Ù„Ù„Ù‡ ÙŠØ§ ØµØ¯Ø§Ù… ÙŠØ§ Ø¨Ø·Ù„ ÙˆÙ…Ù‚Ø¯Ø§Ù…. URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>RT @USER: Ø§Ù†ØªÙˆ Ø¨ØªÙˆØ²Ø¹ÙˆØ§ Ø²ÙŠØª ÙˆØ³ÙƒØ± ÙØ¹Ù„Ø§ ÙŠØ§ Ø¹Ø¨Ø§Ø³ØŸ&lt;...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>RT @USER: ÙƒØ¯Ø§ ÙŠØ§ Ø¹Ù…Ø± Ù…ØªØ²Ø¹Ù„Ù‡Ø§Ø´ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠ ğŸ˜‚ URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>Ù‡Ø¯Ø§ Ø³ÙƒÙ† Ø§Ø·ÙØ§Ù„ Ø§Ù…Ø§Ø±ØªÙŠÙ† Ù…Ù† Ø´Ø§Ø±Ù‚Ø© Ø·Ø§Ù„Ø¨ÙŠÙ† ÙØ²Ø¹ØªÙƒÙ… ÙŠ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>RT @USER: ÙˆÙ…Ø¯Ù†ÙŠ Ø¨Ù…Ø¯Ø¯ Ù…Ù† Ù‚ÙˆØªÙƒ Ø£ÙˆØ§Ø¬Ù‡ Ø¨Ù‡ Ø¶Ø¹ÙÙŠ.. Ùˆ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>ÙŠØ§ Ø³Ù„Ø§Ø§Ø§Ø§Ø§Ù… ÙŠØ§ ÙŠÙˆ Ø®Ø§Ù„Ø¯ Ø§Ù†Øª ÙˆØ§Ù„Ø·Ø±Ø¨ Ø§Ù„Ø§ØµÙŠÙ„ URL</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Tweet Offensive    Hate\n",
              "0    ÙÙ‰ Ø­Ø§Ø¬Ø§Øª Ù…ÙŠÙ†ÙØ¹Ø´ Ù†Ù„ÙØª Ù†Ø¸Ø±ÙƒÙˆØ§ Ù„ÙŠÙ‡Ø§ Ø²Ù‰ Ø§Ù„Ø§ØµÙˆÙ„ ÙƒØ¯Ù‡...   NOT_OFF  NOT_HS\n",
              "1    RT @USER: ÙˆØ¹ÙŠÙˆÙ† ØªÙ†Ø§Ø¯ÙŠÙ†Ø§ ØªØ­Ø§ÙŠÙ„ ÙÙŠÙ†Ø§ Ùˆ Ù†Ù‚ÙˆÙ„ ÙŠØ§ Ø¹...   NOT_OFF  NOT_HS\n",
              "2    ÙŠØ§ Ø¨Ù„Ø§Ø¯ÙŠ ÙŠØ§ Ø£Ù… Ø§Ù„Ø¨Ù„Ø§Ø¯ ÙŠØ§ Ø¨Ù„Ø§Ø¯ÙŠ Ø¨Ø­Ø¨Ùƒ ÙŠØ§ Ù…ØµØ± Ø¨Ø­Ø¨...   NOT_OFF  NOT_HS\n",
              "3    RT @USER: ÙŠØ§ Ø±Ø¨ ÙŠØ§ Ù‚ÙˆÙŠ ÙŠØ§ Ù…Ø¹ÙŠÙ† Ù…Ø¯Ù‘Ù†ÙŠ Ø¨Ø§Ù„Ù‚ÙˆØ© Ùˆ ...   NOT_OFF  NOT_HS\n",
              "4       RT @USER: Ø±Ø­Ù…Ùƒ Ø§Ù„Ù„Ù‡ ÙŠØ§ ØµØ¯Ø§Ù… ÙŠØ§ Ø¨Ø·Ù„ ÙˆÙ…Ù‚Ø¯Ø§Ù…. URL   NOT_OFF  NOT_HS\n",
              "..                                                 ...       ...     ...\n",
              "995  RT @USER: Ø§Ù†ØªÙˆ Ø¨ØªÙˆØ²Ø¹ÙˆØ§ Ø²ÙŠØª ÙˆØ³ÙƒØ± ÙØ¹Ù„Ø§ ÙŠØ§ Ø¹Ø¨Ø§Ø³ØŸ<...   NOT_OFF  NOT_HS\n",
              "996       RT @USER: ÙƒØ¯Ø§ ÙŠØ§ Ø¹Ù…Ø± Ù…ØªØ²Ø¹Ù„Ù‡Ø§Ø´ ÙŠØ§ Ø­Ø¨ÙŠØ¨ÙŠ ğŸ˜‚ URL   NOT_OFF  NOT_HS\n",
              "997  Ù‡Ø¯Ø§ Ø³ÙƒÙ† Ø§Ø·ÙØ§Ù„ Ø§Ù…Ø§Ø±ØªÙŠÙ† Ù…Ù† Ø´Ø§Ø±Ù‚Ø© Ø·Ø§Ù„Ø¨ÙŠÙ† ÙØ²Ø¹ØªÙƒÙ… ÙŠ...   NOT_OFF  NOT_HS\n",
              "998  RT @USER: ÙˆÙ…Ø¯Ù†ÙŠ Ø¨Ù…Ø¯Ø¯ Ù…Ù† Ù‚ÙˆØªÙƒ Ø£ÙˆØ§Ø¬Ù‡ Ø¨Ù‡ Ø¶Ø¹ÙÙŠ.. Ùˆ...   NOT_OFF  NOT_HS\n",
              "999       ÙŠØ§ Ø³Ù„Ø§Ø§Ø§Ø§Ø§Ù… ÙŠØ§ ÙŠÙˆ Ø®Ø§Ù„Ø¯ Ø§Ù†Øª ÙˆØ§Ù„Ø·Ø±Ø¨ Ø§Ù„Ø§ØµÙŠÙ„ URL   NOT_OFF  NOT_HS\n",
              "\n",
              "[1000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9yxPtQjKsPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79622fae-0b4e-4947-8fa1-7503fa4dd578"
      },
      "source": [
        "print(\"Train data shape: {} \\nDev data shape: {}\".format(train_data.shape,dev_data.shape))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape: (7000, 3) \n",
            "Dev data shape: (1000, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "rJB9yBbU5443",
        "outputId": "2701b6f3-32e1-4cd9-eb42-565bbb8f910b"
      },
      "source": [
        "test_data = pd.read_csv('/content/drive/My Drive/Test_data.csv')\r\n",
        "test_data"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Offensive</th>\n",
              "      <th>Hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@USER Ø§Ù…Ø§ Ø§Ù†Øª ØªÙ‚Ø¹Ø¯ Ø·ÙˆÙ„ Ø¹Ù…Ø±Ùƒ Ù„Ø§ Ù…Ø¨Ø¯Ø§ ÙˆÙ„Ø§ Ø±Ø§ÙŠ Ø«Ø§...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@USER @USER Ø¨ØªØ®Ø§Ù Ù†Ø³ÙˆØ§Ù†Ùƒ ÙŠØ²Ø¹Ù„ÙˆØ§ ÙˆÙ„Ø§ Ø§ÙŠÙ‡ ğŸ˜‚ Ø§Ù‡ ÙŠ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RT @USER: ÙŠØ§ Ø¹Ù€Ø³Ø§Ù†Ù€Ù‰ Ù†Ù€Ø¨Ù€Ù‚Ù‰ ÙŠØ§ Ø¹Ù€Ù…Ø±ÙŠ Ø­Ù€Ø¨Ø§ÙŠÙ€Ø¨ Ùˆ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @USER: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù† ÙˆÙŠÙ†Ùˆ Ù…Ø§ Ø´ÙÙ†Ù‡ ÙŠØ§ Ø¨Ø±Ù‡Ø§Ù† &lt;L...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@USER @USER Ø§Ù„Ù„Ù‡Ù… Ø§Ù†Øª Ø§Ù„Ø´Ø§ÙÙŠ Ø§Ù„Ù…Ø¹Ø§ÙÙŠ Ø§Ø´ÙÙŠÙ‡ ÙˆØ¬Ù…...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>RT @USER: Ø§Ù„Ù„Ù‡ Ù„Ø§ÙŠÙˆÙÙ‚Ùƒ ÙŠØ§ Ù…Ù‡Ù†Ø¯ Ø¹Ø³ÙŠØ±ÙŠ ÙŠØ§ Ù…Ø¹ÙˆÙ‚ Ùˆ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>RT @USER: @USER Ø­Ø¨ÙŠØ¨ÙŠ ÙŠØ§ ÙŠÙˆØ³Ù ÙˆØ§Ù†Øª Ø·ÙŠØ¨ ÙŠØ§ ØµØ§Ø­Ø¨...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>RT @USER: ÙŠØ§ Ø¨Ùˆ Ù…Ø­Ù…Ø¯ Ø¹Ø´Øª ÙŠØ§ Ø·ÙŠØ¨ Ø§Ù„ÙØ§Ù„&lt;LF&gt;Ø¹Ø§Ø´Øª ...</td>\n",
              "      <td>NOT_OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>Ø£Ù†Ø§ Ù…Ø³ØªÙ†ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø© Ø¨Ù‚Ø§Ù„ÙŠ Ø³Ù†ØªÙŠÙ† ÙŠØ§ Ø¨Ø¶Ø§Ù† ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>Ø§Ù†ØªØ¸Ø±ÙˆØ§ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø§Ù„Ù‡ÙŠØ© ÙŠØ§ Ù…Ù† ØªØ¯Ø¹Ù…ÙˆÙ† Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ ÙŠ...</td>\n",
              "      <td>OFF</td>\n",
              "      <td>NOT_HS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  Tweet Offensive    Hate\n",
              "0     @USER Ø§Ù…Ø§ Ø§Ù†Øª ØªÙ‚Ø¹Ø¯ Ø·ÙˆÙ„ Ø¹Ù…Ø±Ùƒ Ù„Ø§ Ù…Ø¨Ø¯Ø§ ÙˆÙ„Ø§ Ø±Ø§ÙŠ Ø«Ø§...       OFF      HS\n",
              "1     @USER @USER Ø¨ØªØ®Ø§Ù Ù†Ø³ÙˆØ§Ù†Ùƒ ÙŠØ²Ø¹Ù„ÙˆØ§ ÙˆÙ„Ø§ Ø§ÙŠÙ‡ ğŸ˜‚ Ø§Ù‡ ÙŠ...       OFF  NOT_HS\n",
              "2     RT @USER: ÙŠØ§ Ø¹Ù€Ø³Ø§Ù†Ù€Ù‰ Ù†Ù€Ø¨Ù€Ù‚Ù‰ ÙŠØ§ Ø¹Ù€Ù…Ø±ÙŠ Ø­Ù€Ø¨Ø§ÙŠÙ€Ø¨ Ùˆ...   NOT_OFF  NOT_HS\n",
              "3     RT @USER: Ø¨Ø§Ù‚ÙŠ Ø§Ù„Ø¨ÙŠØ§Ù† ÙˆÙŠÙ†Ùˆ Ù…Ø§ Ø´ÙÙ†Ù‡ ÙŠØ§ Ø¨Ø±Ù‡Ø§Ù† <L...       OFF  NOT_HS\n",
              "4     @USER @USER Ø§Ù„Ù„Ù‡Ù… Ø§Ù†Øª Ø§Ù„Ø´Ø§ÙÙŠ Ø§Ù„Ù…Ø¹Ø§ÙÙŠ Ø§Ø´ÙÙŠÙ‡ ÙˆØ¬Ù…...   NOT_OFF  NOT_HS\n",
              "...                                                 ...       ...     ...\n",
              "1995  RT @USER: Ø§Ù„Ù„Ù‡ Ù„Ø§ÙŠÙˆÙÙ‚Ùƒ ÙŠØ§ Ù…Ù‡Ù†Ø¯ Ø¹Ø³ÙŠØ±ÙŠ ÙŠØ§ Ù…Ø¹ÙˆÙ‚ Ùˆ...       OFF  NOT_HS\n",
              "1996  RT @USER: @USER Ø­Ø¨ÙŠØ¨ÙŠ ÙŠØ§ ÙŠÙˆØ³Ù ÙˆØ§Ù†Øª Ø·ÙŠØ¨ ÙŠØ§ ØµØ§Ø­Ø¨...   NOT_OFF  NOT_HS\n",
              "1997  RT @USER: ÙŠØ§ Ø¨Ùˆ Ù…Ø­Ù…Ø¯ Ø¹Ø´Øª ÙŠØ§ Ø·ÙŠØ¨ Ø§Ù„ÙØ§Ù„<LF>Ø¹Ø§Ø´Øª ...   NOT_OFF  NOT_HS\n",
              "1998  Ø£Ù†Ø§ Ù…Ø³ØªÙ†ÙŠ Ø§Ù„Ø­Ù„Ù‚Ø© Ø¨Ù‚Ø§Ù„ÙŠ Ø³Ù†ØªÙŠÙ† ÙŠØ§ Ø¨Ø¶Ø§Ù† ÙŠØ§ Ø§Ø¨Ù† Ø§Ù„...       OFF  NOT_HS\n",
              "1999  Ø§Ù†ØªØ¸Ø±ÙˆØ§ Ø§Ù„Ø¹Ù‚ÙˆØ¨Ø© Ø§Ù„Ø§Ù„Ù‡ÙŠØ© ÙŠØ§ Ù…Ù† ØªØ¯Ø¹Ù…ÙˆÙ† Ø§Ù„Ø§Ø±Ù‡Ø§Ø¨ ÙŠ...       OFF  NOT_HS\n",
              "\n",
              "[2000 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR9nZxKv6DcW"
      },
      "source": [
        "def get_embedding_matrix(word_index, embedding_index, vocab_dim):\n",
        "    print('Building embedding matrix...')\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, vocab_dim))\n",
        "    for word, i in word_index.items():\n",
        "        try:\n",
        "            embedding_matrix[i] = embedding_index.get_vector(word)\n",
        "        except:\n",
        "            pass\n",
        "    print('Embedding matrix built.') \n",
        "    #print(\"Word index\", word_index.items())\n",
        "    #print(embedding_matrix) \n",
        "    return embedding_matrix\n",
        "\n",
        "\n",
        "def get_init_parameters(path, ext=None):\n",
        "    if ext == 'vec':\n",
        "        word_model = KeyedVectors.load_word2vec_format(path).wv\n",
        "    else:\n",
        "        word_model = KeyedVectors.load(path).wv\n",
        "    n_words = len(word_model.vocab)\n",
        "    vocab_dim = word_model[word_model.index2word[0]].shape[0]\n",
        "    index_dict = dict()\n",
        "    for i in range(n_words):\n",
        "        index_dict[word_model.index2word[i]] = i+1\n",
        "    print('Number of words in the word embedding',n_words)\n",
        "    #print('word_model', word_model)\n",
        "    #print(\"index_dict\",index_dict)\n",
        "    return word_model, index_dict, n_words, vocab_dim\n",
        "\n",
        "def get_max_length(text_data, return_line=False):\n",
        "    max_length = 0\n",
        "    long_line = \"\"\n",
        "    for line in text_data:\n",
        "        new = len(line.split())\n",
        "        if new > max_length:\n",
        "            max_length = new\n",
        "            long_line = line\n",
        "    if return_line:\n",
        "        return long_line, max_length\n",
        "    else:\n",
        "        return max_length\n",
        "    print(\"max\",long_line,max_length)\n",
        "\n",
        "def load_datasets(data_paths, header=True):\n",
        "    x = []\n",
        "    y = []\n",
        "    for data_path in data_paths:\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                if header:\n",
        "                    header = False\n",
        "                else:\n",
        "                    temp = line.split(',')\n",
        "                    x.append(temp[0])\n",
        "                    y.append(temp[2].replace('\\n', ''))\n",
        "    max_length = get_max_length(x)\n",
        "    print('Max length:', max_length)\n",
        "    return x,y, max_length\n",
        "\n",
        "def get_train_test(train_raw_text, dev_raw_text, test_raw_text, n_words, max_length):\n",
        "    tokenizer = text.Tokenizer(num_words=n_words)\n",
        "    tokenizer.fit_on_texts(list(train_raw_text))\n",
        "    word_index = tokenizer.word_index\n",
        "   \n",
        "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
        "    dev_tokenized = tokenizer.texts_to_sequences(dev_raw_text)\n",
        "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
        "\n",
        "    return sequence.pad_sequences(train_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(dev_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           sequence.pad_sequences(test_tokenized, maxlen=max_length, padding='post', truncating='post'),\\\n",
        "           word_index\n",
        "\n",
        "def class_str_2_ind(x_train,x_dev, x_test, y_train,y_dev, y_test, classes, n_words, max_length):\n",
        "    print('Converting data to trainable form...')\n",
        "    y_encoder = preprocessing.LabelEncoder()\n",
        "    y_encoder.fit(classes)\n",
        "    y_train = y_encoder.transform(y_train)\n",
        "    y_dev = y_encoder.transform(y_dev)\n",
        "    y_test = y_encoder.transform(y_test)\n",
        "\n",
        "    #print(y_train)\n",
        "    #print(y_test)\n",
        "    train_y_cat = np_utils.to_categorical(y_train, len(classes))\n",
        "    x_vec_train, x_vec_dev, x_vec_test, word_index = get_train_test(x_train,x_dev, x_test, n_words, max_length)\n",
        "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
        "    print('Number of dev examples: ' + str(len(x_vec_test)))\n",
        "    return x_vec_train,x_vec_dev, x_vec_test, y_train, y_dev, y_test, train_y_cat, word_index\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT0r5MyUbtGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b63a9f-38ed-4596-ada0-f03b1161573c"
      },
      "source": [
        "WORD_MODEL, _, MAX_FEATURES, EMBED_SIZE = get_init_parameters(embedding_path) "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of words in the word embedding 331679\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjwJ3aN3U1Mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1022b7-ebb3-42b0-811c-63d89f46e974"
      },
      "source": [
        "# load train data\n",
        "train_data_path=[\"/content/drive/My Drive/train_data_cleaned.csv\"]\n",
        "x_train, y_train, MAX_TEXT_LENGTH = load_datasets(train_data_path)\n",
        "CLASSES_LIST = np.unique(y_train)\n",
        "print('Label categories: ' + str(CLASSES_LIST))\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 84\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGShWPV6UwCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67816463-5fba-457a-bbb6-1cdb082141a2"
      },
      "source": [
        "# load dev data\n",
        "dev_data_path=[\"/content/drive/My Drive/dev_data_cleaned.csv\"]\n",
        "x_dev, y_dev, MAX_TEXT_LENGTH = load_datasets(dev_data_path)\n",
        "CLASSES_LIST = np.unique(y_dev)\n",
        "print('Label categories: ' + str(CLASSES_LIST))\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 72\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqlf4vZ76KLP",
        "outputId": "aa150d05-ea31-423d-afe9-979a9150a30b"
      },
      "source": [
        "# load test data\r\n",
        "test_data_path=[\"/content/drive/My Drive/test_data_cleaned.csv\"]\r\n",
        "x_test, y_test, MAX_TEXT_LENGTH = load_datasets(test_data_path)\r\n",
        "CLASSES_LIST = np.unique(y_test)\r\n",
        "print('Label categories: ' + str(CLASSES_LIST))\r\n",
        "#0= HS, 1= NOT_HS"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length: 72\n",
            "Label categories: ['HS' 'NOT_HS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcpaFQRJV8YL"
      },
      "source": [
        "MAX_TEXT_LENGTH=84"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plOnSpgUb18i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd9386f-8a1d-4f8d-e1b2-01b5a73c8c52"
      },
      "source": [
        "x_train, x_dev,x_test, y_train, y_dev, y_test, train_y_cat, word_index = class_str_2_ind(x_train, x_dev,x_test, \n",
        "                                                                            y_train, y_dev,y_test,\n",
        "                                                                            CLASSES_LIST, MAX_FEATURES,\n",
        "                                                                            MAX_TEXT_LENGTH)\n",
        "dev_cat_y = np_utils.to_categorical(y_dev, len(CLASSES_LIST))\n",
        "test_cat_y = np_utils.to_categorical(y_test, len(CLASSES_LIST))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting data to trainable form...\n",
            "Number of training examples: 7000\n",
            "Number of dev examples: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQiq-BKShbLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "371c2590-f9be-4153-a87e-56adc629a0da"
      },
      "source": [
        "print(\"Tokens number: \"+str(len(word_index)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokens number: 30103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjJd-CWUiNlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efa0fc6b-3e50-4ab1-a142-1ee722c1dc96"
      },
      "source": [
        "# Sequence length\n",
        "print(\"Original sequence length: \"+str(MAX_TEXT_LENGTH))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original sequence length: 84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXB7MDh9lLcM"
      },
      "source": [
        "#  CNN model building:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRIAZtDUidul"
      },
      "source": [
        "def get_model(embedding_weights, word_index, vocab_dim, max_length, print_summary=True):\n",
        "    \"\"\"\n",
        "    Create Neural Network With an Embedding layer\n",
        "    \"\"\"\n",
        "    inp = Input(shape=(max_length,))\n",
        "    model = Embedding(input_dim=len(word_index)+1,\n",
        "                      output_dim=vocab_dim,\n",
        "                      trainable=False,\n",
        "                      weights=[embedding_weights])(inp)\n",
        "\n",
        "    model = Conv1D(filters=25, kernel_size=5, padding='same', activation='relu')(model)\n",
        "    model = MaxPooling1D(pool_size=2)(model)\n",
        "    model = Flatten()(model)\n",
        "   \n",
        "    model = Dense(2, activation='sigmoid')(model)\n",
        "    model = Model(inputs=inp, outputs=model)\n",
        "    \n",
        "    from keras import optimizers\n",
        "\n",
        "    opt = optimizers.Adam(lr=0.0001)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    if print_summary:\n",
        "        model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH):\n",
        "    tmp = get_embedding_matrix(word_index, WORD_MODEL, EMBED_SIZE)\n",
        "    model = get_model(tmp, word_index, EMBED_SIZE, MAX_TEXT_LENGTH, print_summary=True)\n",
        "    return model\n",
        "\n",
        "\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        self.test_data = test_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x, y = self.test_data\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "def train_fit_predict(model, x_train, x_dev, y_train, y_dev, batch_size, epochs, TestCallback=TestCallback):\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, verbose=1,\n",
        "                        validation_data=(x_dev, y_dev),\n",
        "                        callbacks=[TestCallback((x_dev, y_dev))])\n",
        "    return history, model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKpEhF4ljGqR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d7b591-1751-4a0e-a47b-de3135b636fa"
      },
      "source": [
        "model = get_main_model(word_index, WORD_MODEL, EMBED_SIZE, MAX_TEXT_LENGTH)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building embedding matrix...\n",
            "Embedding matrix built.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 84)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 84, 300)           9031200   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 84, 25)            37525     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 42, 25)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1050)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 2102      \n",
            "=================================================================\n",
            "Total params: 9,070,827\n",
            "Trainable params: 39,627\n",
            "Non-trainable params: 9,031,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIdYkbZzjJpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dda89fb-7d03-434d-fd1a-3f5751f35be8"
      },
      "source": [
        "time_start = time()\n",
        "history, model = train_fit_predict(model,\n",
        "                               x_train[:, :MAX_TEXT_LENGTH],\n",
        "                               x_dev[:, :MAX_TEXT_LENGTH],\n",
        "                               train_y_cat, dev_cat_y,\n",
        "                               batch_size=500, epochs=10)\n",
        "time_start = time() - time_start\n",
        "\n",
        "print(\"Took : \"+str(np.round(time_start, 2))+\" (s)\") "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 6s 371ms/step - loss: 0.6279 - accuracy: 0.7688 - val_loss: 0.5042 - val_accuracy: 0.9550\n",
            "\n",
            "Testing loss: 0.5041759014129639, acc: 0.9549999833106995\n",
            "\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 5s 363ms/step - loss: 0.4603 - accuracy: 0.9487 - val_loss: 0.3747 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.37473323941230774, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 5s 367ms/step - loss: 0.3551 - accuracy: 0.9443 - val_loss: 0.2939 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.2939288020133972, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2844 - accuracy: 0.9501 - val_loss: 0.2487 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.24870163202285767, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 5s 359ms/step - loss: 0.2532 - accuracy: 0.9478 - val_loss: 0.2243 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.22430956363677979, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2326 - accuracy: 0.9484 - val_loss: 0.2094 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.20937475562095642, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 5s 364ms/step - loss: 0.2131 - accuracy: 0.9511 - val_loss: 0.1992 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.19916605949401855, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 5s 365ms/step - loss: 0.2054 - accuracy: 0.9501 - val_loss: 0.1917 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.1916774958372116, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 5s 362ms/step - loss: 0.2005 - accuracy: 0.9481 - val_loss: 0.1860 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.18595744669437408, acc: 0.9559999704360962\n",
            "\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 5s 363ms/step - loss: 0.1914 - accuracy: 0.9495 - val_loss: 0.1816 - val_accuracy: 0.9560\n",
            "\n",
            "Testing loss: 0.1815880835056305, acc: 0.9559999704360962\n",
            "\n",
            "Took : 54.73 (s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt-l_Q9Aj44w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45507edc-458a-49d6-c556-ad0e1ed9b78a"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsxi8A1j_Fq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d59db165-b1b0-4413-c136-7fac4790e105"
      },
      "source": [
        "model.evaluate(x_dev[:, :MAX_TEXT_LENGTH], dev_cat_y, batch_size=1000)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 226ms/step - loss: 0.1816 - accuracy: 0.9560\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1815880984067917, 0.9559999704360962]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1e78k51kZ1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e783ecf4-5d85-4401-dd9f-d71015605fec"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_dev[:, :MAX_TEXT_LENGTH]), axis=1)\n",
        "\n",
        "print(creport(np.argmax(dev_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=3))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          HS      0.000     0.000     0.000        44\n",
            "      NOT_HS      0.956     1.000     0.978       956\n",
            "\n",
            "    accuracy                          0.956      1000\n",
            "   macro avg      0.478     0.500     0.489      1000\n",
            "weighted avg      0.914     0.956     0.934      1000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq27ypCT6dgw",
        "outputId": "6662dd8a-fbd6-42fe-e47b-b1e0da4ce7b4"
      },
      "source": [
        "model.evaluate(x_test[:, :MAX_TEXT_LENGTH], test_cat_y, batch_size=1000)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 212ms/step - loss: 0.1948 - accuracy: 0.9495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.19483204185962677, 0.9495000243186951]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKxgtpLZ6dot",
        "outputId": "a3ec310d-ce4b-4153-85c4-6f996e9c53b5"
      },
      "source": [
        "y_pred = np.argmax(model.predict(x_test[:, :MAX_TEXT_LENGTH]), axis=1)\r\n",
        "\r\n",
        "print(creport(np.argmax(test_cat_y, axis=1), y_pred,target_names=['HS', 'NOT_HS'],digits=3))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          HS      0.000     0.000     0.000       101\n",
            "      NOT_HS      0.950     1.000     0.974      1899\n",
            "\n",
            "    accuracy                          0.950      2000\n",
            "   macro avg      0.475     0.500     0.487      2000\n",
            "weighted avg      0.902     0.950     0.925      2000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po5GA7KMkGUM"
      },
      "source": [
        "n = np.argmin(history.history['val_loss'])\n",
        "\n",
        "print(\"Optimal epoch : {}\".format(n))\n",
        "print(\"Accuracy on train : {} %\".format(np.round(history.history['accuracy'][n]*100, 2)))\n",
        "print(\"Accuracy on val : {} %\".format(np.round(history.history['val_accuracy'][n]*100, 2)))\n",
        "print(\"Loss on train : {}\".format(np.round(history.history['loss'][n]*100, 2)))\n",
        "print(\"Loss on Val : {}\".format(np.round(history.history['val_loss'][n]*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlVrg_zJkkLx"
      },
      "source": [
        "plt.figure(\"Loss Plot\", figsize=(12, 6))\n",
        "plt.plot(range(1, len(history.history['loss'])+1), history.history['loss'], label=\"train loss\")\n",
        "plt.plot(range(1, len(history.history['val_loss'])+1), history.history['val_loss'], label=\"val loss\")\n",
        "plt.plot(n+1,history.history[\"val_loss\"][n],\"r*\", label=\"Lowest loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.ylabel(\"loss (cross_entropy)\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZp0EW06kwhy"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model, to_file='RNN_LSTM_model.png', show_shapes=False, show_layer_names=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMeeL-Gqfy6p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}